{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9cbc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superorange5/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:803: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Nb_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9f84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import PascalVOCDetectionEvaluator\n",
    "from detectron2.evaluation import inference_on_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41308c13",
   "metadata": {},
   "source": [
    "# 0.Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b5dc6",
   "metadata": {},
   "source": [
    "## 0.a skf2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97bd4561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_experiments/s2c/cfg.yaml\n",
      "keep_experiments/s2c/model_0015999.pth\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "keep_experiments/k2c/cfg.yaml\n",
      "keep_experiments/k2c/model_0019999.pth\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "keep_experiments/f2c_1class/cfg.yaml\n",
      "keep_experiments/f2c_1class/model_0011999.pth\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---------skf2c-------\n",
    "model_s2c = get_model('s2c', 15999)\n",
    "model_k2c = get_model('k2c', 19999)\n",
    "#model_f2c = get_model('f2c', 'final') #8class\n",
    "model_f2c = get_model('f2c_1class', 11999)\n",
    "model_list=[model_s2c,model_k2c,model_f2c]\n",
    "\n",
    "\n",
    "cfg = setup(\"configs/pt/final_k2c.yaml\")\n",
    "dataset_name = 'VOC2007_cityval1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ea23f",
   "metadata": {},
   "source": [
    "## 0.b ck2bdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d674edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_experiments/c2b/cfg.yaml\n",
      "keep_experiments/c2b/model_0011999.pth\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "keep_experiments/k2b/cfg.yaml\n",
      "keep_experiments/k2b/model_final.pth\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    }
   ],
   "source": [
    "#--------ck2bdd-------\n",
    "model_c2b = get_model('c2b', 11999)\n",
    "model_k2b = get_model('k2b', 'final')\n",
    "model_list=[model_c2b,model_k2b]\n",
    "\n",
    "cfg = setup(\"keep_experiments/c2b/cfg.yaml\")\n",
    "dataset_name = 'VOC2007_bddval1' #small'\n",
    "#dataset_name = 'VOC2007_bddvalsmall'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd79a4",
   "metadata": {},
   "source": [
    "## 0.c single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "518072ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- pretrained model loaded ---------\n"
     ]
    }
   ],
   "source": [
    "#model_to_eval = load_sourceonlyModel(\"keep_experiments/c2b/cfg.yaml\", \"output/FedAvg_ck2b/FedAvg_2.pth\")\n",
    "model_to_eval = load_sourceonlyModel(\"keep_experiments/s2c/cfg.yaml\", \"./output/FedAvg_skf2c_sourceonly/FedAvg_2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301d22f",
   "metadata": {},
   "source": [
    "* load TS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3733f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block1.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block1.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block2.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block2.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block3.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block3.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.backbone.vgg_block5.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.proposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.proposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.proposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.roi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.roi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.roi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mmodelTeacher.roi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block1.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block1.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block2.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block2.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block3.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block3.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.backbone.vgg_block5.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.proposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.proposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.proposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.roi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.roi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.roi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.0.roi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block1.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block1.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block2.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block2.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block3.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block3.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.backbone.vgg_block5.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.proposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.proposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.proposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.roi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.roi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.roi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.1.roi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block1.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block1.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block2.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block2.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block3.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block3.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.backbone.vgg_block5.0.conv3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.proposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.proposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.proposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.roi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.roi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.roi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "  \u001b[35mmodelTeacher.2.roi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_to_eval =load_TSmodel(\"keep_experiments/s2c/cfg.yaml\", \"output/multi-teacher_skf2c_foggy_sourceonly/model_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43162640",
   "metadata": {},
   "source": [
    "* load FedMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afa48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pt.engine.trainer_sourceonly import PTrainer_sourceonly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3163e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_backbone_shape(fedma_model):\n",
    "    backbone_shape = []\n",
    "    for idx,(key,value) in enumerate(fedma_model.items()):\n",
    "        if idx%2!=0 and idx<26:\n",
    "            backbone_shape.append(list(value.shape)[0])\n",
    "    return backbone_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be15ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fedma_model(model, cfg_path, source_dataset):\n",
    "\n",
    "    backbone_dim = get_backbone_shape(model)\n",
    "    print(backbone_dim)\n",
    "\n",
    "    # build FedMA dynamic backbone model\n",
    "    cfg = setup_all(cfg_path)\n",
    "    cfg.defrost()\n",
    "    cfg.MODEL.WEIGHTS = model_path                \n",
    "    cfg.DATASETS.TRAIN_LABEL=source_dataset\n",
    "    print(\"current source={}\".format(source_dataset))\n",
    "    cfg.BACKBONE_DIM = backbone_dim\n",
    "    cfg.freeze()\n",
    "\n",
    "    Trainer =PTrainer_sourceonly\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    return trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49326774",
   "metadata": {},
   "source": [
    "* load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ef4b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    }
   ],
   "source": [
    "model_path = 'output/FedMA_skf2c_source-only/VOC2007_kitti1_6/model_final.pth'\n",
    "fedma_model = torch.load(model_path)\n",
    "cfg_path = \"configs/FedMA/ck2b_FedMA.yaml\"\n",
    "source_dataset = \"VOC2007_kitti1\"\n",
    "model_to_eval = build_fedma_model(fedma_model['model'], cfg_path, source_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae8b76",
   "metadata": {},
   "source": [
    "* load fedma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac4801bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 64, 128, 128, 256, 262, 262, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    }
   ],
   "source": [
    "model_path = 'output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_7.pth'\n",
    "fedma_model = torch.load(model_path)\n",
    "cfg_path = \"configs/FedMA/ck2b_FedMA.yaml\"\n",
    "source_dataset = \"VOC2007_kitti1\"\n",
    "model_to_eval = build_fedma_model(fedma_model, cfg_path, source_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c2d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'VOC2007_cityval1'\n",
    "cfg = setup_all(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "161c58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_fedma(fedma_model_path,cfg_path, source_dataset, eval_dataset):  \n",
    "    print(fedma_model_path)\n",
    "    fedma_model = torch.load(fedma_model_path)\n",
    "    model_to_eval = build_fedma_model(fedma_model, cfg_path, source_dataset)\n",
    "    \n",
    "    cfg = setup_all(cfg_path)\n",
    "    test_data_loader = build_detection_test_loader(cfg, eval_dataset)\n",
    "    evaluator = PascalVOCDetectionEvaluator(dataset_name)\n",
    "    \n",
    "    results_i = inference_on_dataset(model_to_eval, test_data_loader, evaluator)\n",
    "    return results_i['bbox']['AP50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24e400e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC2007_sim1\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_6.pth\n",
      "[64, 64, 128, 128, 256, 262, 256, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.176766205840426\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_7.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([512, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (512, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011457742374234652\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_8.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015225417673574318\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_9.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 512, 512, 512, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000139752083181424\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_10.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 512, 512, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_11.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 518, 512, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv2.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv2.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016194641394652407\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_sim1_12.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 518, 518, 512]\n",
      "current source=VOC2007_sim1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv3.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv3.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "VOC2007_kitti1\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_6.pth\n",
      "[64, 64, 128, 128, 256, 262, 256, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.176766205840426\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_7.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([512, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (512, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002406479206906193\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_8.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_9.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 512, 512, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09171739507332141\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_10.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 512, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00022855341490837336\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_11.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 518, 512, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv2.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv2.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_kitti1_12.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 518, 518, 512]\n",
      "current source=VOC2007_kitti1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv3.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv3.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "VOC2007_foggytrain1\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_6.pth\n",
      "[64, 64, 128, 128, 256, 262, 256, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.176766205840426\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_7.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 512, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([512, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (512, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011362782266085415\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_8.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 512, 512, 512, 512, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00047615646501440376\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_9.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 512, 512, 512, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010295026507286822\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_10.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 512, 512, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_11.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 518, 512, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv2.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv2.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "output/FedMA_skf2c_source-only/FedMA_VOC2007_foggytrain1_12.pth\n",
      "[64, 64, 128, 128, 256, 262, 262, 518, 518, 518, 518, 518, 512]\n",
      "current source=VOC2007_foggytrain1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n",
      "Shape of backbone.vgg_block3.0.conv3.bias in checkpoint is torch.Size([256]), while shape of backbone.vgg_block3.0.conv3.bias in model is torch.Size([262]).\n",
      "backbone.vgg_block3.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block3.0.conv3.weight in checkpoint is torch.Size([256, 262, 3, 3]), while shape of backbone.vgg_block3.0.conv3.weight in model is torch.Size([262, 262, 3, 3]).\n",
      "backbone.vgg_block3.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv1.weight in checkpoint is torch.Size([512, 256, 3, 3]), while shape of backbone.vgg_block4.0.conv1.weight in model is torch.Size([518, 262, 3, 3]).\n",
      "backbone.vgg_block4.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block4.0.conv3.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block4.0.conv3.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block4.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block4.0.conv3.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block4.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv1.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv1.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv1.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv1.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv1.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.bias in checkpoint is torch.Size([512]), while shape of backbone.vgg_block5.0.conv2.bias in model is torch.Size([518]).\n",
      "backbone.vgg_block5.0.conv2.bias will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv2.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv2.weight in model is torch.Size([518, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv2.weight will not be loaded. Please double check and see if this is desired.\n",
      "Shape of backbone.vgg_block5.0.conv3.weight in checkpoint is torch.Size([512, 512, 3, 3]), while shape of backbone.vgg_block5.0.conv3.weight in model is torch.Size([512, 518, 3, 3]).\n",
      "backbone.vgg_block5.0.conv3.weight will not be loaded. Please double check and see if this is desired.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.bias' to the model due to incompatible shapes: (256,) in the checkpoint but (262,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block3.0.conv3.weight' to the model due to incompatible shapes: (256, 262, 3, 3) in the checkpoint but (262, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv1.weight' to the model due to incompatible shapes: (512, 256, 3, 3) in the checkpoint but (518, 262, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block4.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv1.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.bias' to the model due to incompatible shapes: (512,) in the checkpoint but (518,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv2.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (518, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'backbone.vgg_block5.0.conv3.weight' to the model due to incompatible shapes: (512, 512, 3, 3) in the checkpoint but (512, 518, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.vgg_block3.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block4.0.conv3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.vgg_block5.0.conv3.weight\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_dataset_list=[\"VOC2007_sim1\",\"VOC2007_kitti1\",\"VOC2007_foggytrain1\" ]\n",
    "\n",
    "cfg_path = \"configs/FedMA/ck2b_FedMA.yaml\"\n",
    "eval_dataset = 'VOC2007_cityval1'\n",
    "for dataset in source_dataset_list:\n",
    "    print(dataset)\n",
    "    for i in range(6,13):\n",
    "        fedma_model_path = 'output/FedMA_skf2c_source-only/FedMA_{}_{}.pth'.format(dataset,i)\n",
    "        AP50 = eval_fedma(fedma_model_path,cfg_path, dataset, eval_dataset)\n",
    "        print(AP50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a09dc",
   "metadata": {},
   "source": [
    "## evaluation (defaults.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dcd5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = OrderedDict()\n",
    "test_data_loader = build_detection_test_loader(cfg, dataset_name)\n",
    "evaluator = PascalVOCDetectionEvaluator(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654f98f",
   "metadata": {},
   "source": [
    "### generate original car.txt (change code in detectron2/evaluation/pascal_voc_evaluation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aec0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_c2b.eval()\n",
    "#results_i = inference_on_dataset(model_k2b.modelStudent, test_data_loader, evaluator)\n",
    "results_i = inference_on_dataset(model_to_eval, test_data_loader, evaluator)\n",
    "\n",
    "#tmp file=/tmp/pascal_voc_eval_kzuu496e/{}.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "374f2ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bbox',\n",
       "              {'AP': 10.63140269660908,\n",
       "               'AP50': 29.176766205840426,\n",
       "               'AP75': 4.7804266817687475})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f99aef",
   "metadata": {},
   "source": [
    "## load car.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c9377",
   "metadata": {},
   "source": [
    "* print out information\n",
    "  * self._anno_file_template=data/VOC2007_bddval/Annotations/{}.xml\n",
    "  * self._image_set_path=data/VOC2007_bddval/ImageSets/Main/val_small.txt\n",
    "  * self._is_2007=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1331ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_is_2007 = False\n",
    "res_file_template = \"tmp/skf2c_threshold_foggy/{}.txt\"\n",
    "\n",
    "cls_name = 'car'\n",
    "\n",
    "#------city\n",
    "_anno_file_template = \"data/VOC2007_cityval/Annotations/{}.xml\"\n",
    "_image_set_path = \"data/VOC2007_cityval/ImageSets/Main/val.txt\"\n",
    "\n",
    "#------bdd\n",
    "#_anno_file_template = \"data/VOC2007_bddval/Annotations/{}.xml\"\n",
    "#_image_set_path = \"data/VOC2007_bddval/ImageSets/Main/val.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4a511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation.pascal_voc_evaluation import voc_eval\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "aps = defaultdict(list)\n",
    "\n",
    "for thresh in range(50, 100, 5):\n",
    "    rec, prec, ap = voc_eval(\n",
    "        res_file_template,\n",
    "        _anno_file_template,\n",
    "        _image_set_path,\n",
    "        cls_name,\n",
    "        ovthresh=thresh / 100.0,\n",
    "        use_07_metric=_is_2007,\n",
    "    )\n",
    "    \n",
    "    aps[thresh].append(ap * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1886dd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {50: [63.90187105980007],\n",
       "             55: [61.124466035304145],\n",
       "             60: [58.239042198706706],\n",
       "             65: [54.56696503077557],\n",
       "             70: [50.04824192967497],\n",
       "             75: [44.60209282320927],\n",
       "             80: [38.23068266882637],\n",
       "             85: [29.358496451569238],\n",
       "             90: [17.93286038737506],\n",
       "             95: [2.0180323047458235]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3828a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_c2b.modelStudent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c34abbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_result=[]\n",
    "    for model in model_list_ck2b:\n",
    "        modelStudent= model.modelStudent\n",
    "        modelStudent.eval()\n",
    "        output=modelStudent(data)\n",
    "        output_result.append(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7d5b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9782, 0.9770, 0.9530, 0.9435, 0.9127, 0.9055, 0.9023, 0.9006, 0.8991,\n",
       "        0.8486, 0.7825, 0.7520, 0.3332, 0.2876, 0.2741, 0.2580, 0.2209, 0.2179,\n",
       "        0.1990, 0.1814, 0.1572, 0.1552, 0.1415, 0.1287, 0.0757, 0.0703, 0.0678,\n",
       "        0.0633, 0.0492, 0.0355, 0.0342, 0.0307, 0.0280, 0.0252, 0.0211, 0.0204,\n",
       "        0.0202, 0.0162, 0.0151, 0.0113, 0.0109, 0.0101, 0.0090, 0.0088, 0.0078,\n",
       "        0.0048, 0.0043, 0.0022, 0.0012], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_result[1][0]['instances'].scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d7fa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.reset()\n",
    "evaluator.process(data, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3446926b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['b1c66a42-6f7d68ca 0.971 933.1 339.5 1221.0 481.7',\n",
       "              'b1c66a42-6f7d68ca 0.971 796.0 363.4 905.5 430.0',\n",
       "              'b1c66a42-6f7d68ca 0.961 751.1 363.6 820.0 413.1',\n",
       "              'b1c66a42-6f7d68ca 0.946 39.8 351.0 127.8 400.2',\n",
       "              'b1c66a42-6f7d68ca 0.931 882.3 378.1 966.8 448.7',\n",
       "              'b1c66a42-6f7d68ca 0.899 247.1 350.0 349.4 397.7',\n",
       "              'b1c66a42-6f7d68ca 0.896 599.8 355.0 635.8 385.3',\n",
       "              'b1c66a42-6f7d68ca 0.877 1.0 341.4 58.4 403.1',\n",
       "              'b1c66a42-6f7d68ca 0.875 733.7 367.5 771.7 404.4',\n",
       "              'b1c66a42-6f7d68ca 0.756 684.1 360.5 718.3 391.1',\n",
       "              'b1c66a42-6f7d68ca 0.753 1.0 335.7 10.8 391.8',\n",
       "              'b1c66a42-6f7d68ca 0.628 204.9 344.2 258.1 386.0',\n",
       "              'b1c66a42-6f7d68ca 0.594 215.8 359.8 300.0 455.8',\n",
       "              'b1c66a42-6f7d68ca 0.539 709.3 365.5 743.6 398.0',\n",
       "              'b1c66a42-6f7d68ca 0.457 692.2 362.0 737.2 395.2',\n",
       "              'b1c66a42-6f7d68ca 0.418 1204.3 418.6 1279.2 524.1',\n",
       "              'b1c66a42-6f7d68ca 0.257 13.6 346.9 101.7 403.0',\n",
       "              'b1c66a42-6f7d68ca 0.200 587.3 351.0 612.7 376.9',\n",
       "              'b1c66a42-6f7d68ca 0.185 593.7 353.9 622.5 380.1',\n",
       "              'b1c66a42-6f7d68ca 0.181 962.9 256.1 1032.8 309.5',\n",
       "              'b1c66a42-6f7d68ca 0.135 568.2 336.4 607.9 373.0',\n",
       "              'b1c66a42-6f7d68ca 0.112 1.3 346.4 23.2 405.4',\n",
       "              'b1c66a42-6f7d68ca 0.076 230.0 350.6 340.0 426.8',\n",
       "              'b1c66a42-6f7d68ca 0.039 403.3 357.8 438.2 376.7',\n",
       "              'b1c66a42-6f7d68ca 0.033 368.8 354.4 408.5 377.2',\n",
       "              'b1c66a42-6f7d68ca 0.031 672.0 362.1 699.9 387.5',\n",
       "              'b1c66a42-6f7d68ca 0.021 708.3 355.7 757.2 394.3',\n",
       "              'b1c66a42-6f7d68ca 0.018 648.2 363.8 678.3 386.2',\n",
       "              'b1c66a42-6f7d68ca 0.017 363.7 351.7 429.2 379.9',\n",
       "              'b1c66a42-6f7d68ca 0.016 333.5 356.9 374.2 383.0',\n",
       "              'b1c66a42-6f7d68ca 0.014 447.8 357.9 479.6 374.0',\n",
       "              'b1c66a42-6f7d68ca 0.013 233.1 365.1 297.7 417.9',\n",
       "              'b1c66a42-6f7d68ca 0.012 204.1 343.2 308.9 397.0',\n",
       "              'b1c66a42-6f7d68ca 0.011 413.4 359.5 453.1 377.7',\n",
       "              'b1c66a42-6f7d68ca 0.010 353.6 350.6 396.0 374.7',\n",
       "              'b1c66a42-6f7d68ca 0.009 158.2 328.5 238.2 376.3']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator._predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14c2f2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['instances'].image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "901d3521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9708, 0.9707, 0.9612, 0.9462, 0.9312, 0.8988, 0.8963, 0.8774, 0.8749,\n",
       "        0.7561, 0.7531, 0.6275, 0.5939, 0.5390, 0.4565, 0.4178, 0.2566, 0.1998,\n",
       "        0.1848, 0.1812, 0.1348, 0.1121, 0.0759, 0.0390, 0.0330, 0.0310, 0.0206,\n",
       "        0.0180, 0.0171, 0.0161, 0.0135, 0.0127, 0.0123, 0.0106, 0.0099, 0.0094],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['instances'].pred_boxes\n",
    "output[0]['instances'].scores_logists\n",
    "output[0]['instances'].boxes_sigma\n",
    "output[0]['instances'].scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5cf79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21073/4102160977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "len(output[0]['instances'].scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "13f0e875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Instances.get_fields of Instances(num_instances=36, image_height=720, image_width=1280, fields=[pred_boxes: Boxes(tensor([[9.3210e+02, 3.3852e+02, 1.2210e+03, 4.8170e+02],\n",
       "        [7.9497e+02, 3.6244e+02, 9.0552e+02, 4.3002e+02],\n",
       "        [7.5011e+02, 3.6257e+02, 8.2003e+02, 4.1306e+02],\n",
       "        [3.8800e+01, 3.4999e+02, 1.2780e+02, 4.0018e+02],\n",
       "        [8.8130e+02, 3.7710e+02, 9.6676e+02, 4.4869e+02],\n",
       "        [2.4614e+02, 3.4897e+02, 3.4941e+02, 3.9769e+02],\n",
       "        [5.9876e+02, 3.5402e+02, 6.3575e+02, 3.8532e+02],\n",
       "        [0.0000e+00, 3.4040e+02, 5.8351e+01, 4.0309e+02],\n",
       "        [7.3273e+02, 3.6651e+02, 7.7169e+02, 4.0439e+02],\n",
       "        [6.8310e+02, 3.5948e+02, 7.1826e+02, 3.9106e+02],\n",
       "        [0.0000e+00, 3.3469e+02, 1.0768e+01, 3.9184e+02],\n",
       "        [2.0393e+02, 3.4324e+02, 2.5814e+02, 3.8604e+02],\n",
       "        [2.1476e+02, 3.5879e+02, 3.0001e+02, 4.5576e+02],\n",
       "        [7.0833e+02, 3.6453e+02, 7.4357e+02, 3.9798e+02],\n",
       "        [6.9122e+02, 3.6096e+02, 7.3717e+02, 3.9521e+02],\n",
       "        [1.2033e+03, 4.1763e+02, 1.2792e+03, 5.2414e+02],\n",
       "        [1.2558e+01, 3.4589e+02, 1.0172e+02, 4.0303e+02],\n",
       "        [5.8631e+02, 3.5004e+02, 6.1269e+02, 3.7690e+02],\n",
       "        [5.9269e+02, 3.5286e+02, 6.2253e+02, 3.8014e+02],\n",
       "        [9.6188e+02, 2.5513e+02, 1.0328e+03, 3.0948e+02],\n",
       "        [5.6717e+02, 3.3535e+02, 6.0787e+02, 3.7301e+02],\n",
       "        [2.6411e-01, 3.4545e+02, 2.3222e+01, 4.0542e+02],\n",
       "        [2.2899e+02, 3.4963e+02, 3.3996e+02, 4.2685e+02],\n",
       "        [4.0234e+02, 3.5680e+02, 4.3816e+02, 3.7669e+02],\n",
       "        [3.6780e+02, 3.5338e+02, 4.0846e+02, 3.7721e+02],\n",
       "        [6.7104e+02, 3.6107e+02, 6.9990e+02, 3.8752e+02],\n",
       "        [7.0729e+02, 3.5469e+02, 7.5724e+02, 3.9433e+02],\n",
       "        [6.4716e+02, 3.6279e+02, 6.7828e+02, 3.8616e+02],\n",
       "        [3.6273e+02, 3.5067e+02, 4.2916e+02, 3.7986e+02],\n",
       "        [3.3248e+02, 3.5591e+02, 3.7417e+02, 3.8297e+02],\n",
       "        [4.4679e+02, 3.5686e+02, 4.7964e+02, 3.7400e+02],\n",
       "        [2.3211e+02, 3.6414e+02, 2.9770e+02, 4.1787e+02],\n",
       "        [2.0311e+02, 3.4216e+02, 3.0886e+02, 3.9695e+02],\n",
       "        [4.1240e+02, 3.5850e+02, 4.5306e+02, 3.7772e+02],\n",
       "        [3.5265e+02, 3.4964e+02, 3.9602e+02, 3.7467e+02],\n",
       "        [1.5720e+02, 3.2750e+02, 2.3821e+02, 3.7633e+02]], device='cuda:0')), scores: tensor([0.9708, 0.9707, 0.9612, 0.9462, 0.9312, 0.8988, 0.8963, 0.8774, 0.8749,\n",
       "        0.7561, 0.7531, 0.6275, 0.5939, 0.5390, 0.4565, 0.4178, 0.2566, 0.1998,\n",
       "        0.1848, 0.1812, 0.1348, 0.1121, 0.0759, 0.0390, 0.0330, 0.0310, 0.0206,\n",
       "        0.0180, 0.0171, 0.0161, 0.0135, 0.0127, 0.0123, 0.0106, 0.0099, 0.0094],\n",
       "       device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), scores_logists: tensor([[ 3.3746, -3.4222],\n",
       "        [ 3.0637, -3.1017],\n",
       "        [ 3.7150, -3.7093],\n",
       "        [ 3.2326, -3.2229],\n",
       "        [ 2.6753, -2.6198],\n",
       "        [ 2.1805, -2.2141],\n",
       "        [ 2.9142, -2.8905],\n",
       "        [ 2.7713, -2.7547],\n",
       "        [ 2.6244, -2.6060],\n",
       "        [ 1.7956, -1.8262],\n",
       "        [ 1.2354, -1.2762],\n",
       "        [ 1.0564, -1.0052],\n",
       "        [ 0.5821, -0.5865],\n",
       "        [ 2.3656, -2.3056],\n",
       "        [ 1.9363, -1.8746],\n",
       "        [ 0.0897, -0.0704],\n",
       "        [ 0.1624, -0.1088],\n",
       "        [ 0.3955, -0.3212],\n",
       "        [-0.0378,  0.0782],\n",
       "        [-0.4594,  0.4989],\n",
       "        [ 0.0195,  0.0350],\n",
       "        [-0.5129,  0.6021],\n",
       "        [-0.6291,  0.5911],\n",
       "        [-0.6431,  0.6644],\n",
       "        [-0.7335,  0.7555],\n",
       "        [-1.1361,  1.1194],\n",
       "        [-1.2835,  1.2587],\n",
       "        [-1.4107,  1.3891],\n",
       "        [-1.1443,  1.1682],\n",
       "        [-1.3804,  1.4071],\n",
       "        [-1.2687,  1.2785],\n",
       "        [-1.1065,  1.0902],\n",
       "        [-1.2292,  1.2823],\n",
       "        [-1.4174,  1.4435],\n",
       "        [-1.2412,  1.2587],\n",
       "        [-1.2616,  1.3054]], device='cuda:0'), boxes_sigma: tensor([[-3.4894e+00, -3.9871e+00, -3.1702e+00, -3.6849e+00],\n",
       "        [-3.6285e+00, -3.7810e+00, -3.3161e+00, -3.6317e+00],\n",
       "        [-3.2193e+00, -3.3684e+00, -2.9709e+00, -3.3978e+00],\n",
       "        [-2.8282e+00, -3.2153e+00, -2.5468e+00, -3.1238e+00],\n",
       "        [-2.9457e+00, -2.7821e+00, -2.3568e+00, -2.7212e+00],\n",
       "        [-2.2454e+00, -2.4638e+00, -2.3135e+00, -2.2387e+00],\n",
       "        [-2.2998e+00, -2.2267e+00, -2.0085e+00, -2.2291e+00],\n",
       "        [-1.8328e+00, -2.2246e+00, -1.6705e+00, -2.4066e+00],\n",
       "        [-1.6395e+00, -2.2094e+00, -1.8167e+00, -2.4366e+00],\n",
       "        [-6.3844e-01, -1.7724e+00, -9.5738e-01, -1.9336e+00],\n",
       "        [-1.0417e+00, -2.0666e+00, -1.1329e+00, -1.9339e+00],\n",
       "        [-1.2190e+00, -8.2443e-01, -8.5451e-01, -6.6751e-01],\n",
       "        [-2.6278e+00, -8.5548e-01, -1.8027e+00, -4.9464e-01],\n",
       "        [ 2.1113e+00, -1.0854e+00,  2.1824e-01, -1.9466e+00],\n",
       "        [ 2.8271e+00, -7.2808e-01,  8.0095e-01, -1.5608e+00],\n",
       "        [-1.6114e+00, -1.4486e+00, -8.4586e-01, -1.1116e+00],\n",
       "        [ 3.1729e+00, -1.0267e+00,  1.2308e+00, -1.4266e+00],\n",
       "        [ 3.1314e+00,  8.1953e-01,  1.2376e+00, -4.7690e-01],\n",
       "        [ 2.4866e+00,  1.3186e-01,  7.3442e-01, -8.5484e-01],\n",
       "        [-1.1308e+00, -7.3917e-01, -7.5899e-01, -3.8568e-03],\n",
       "        [ 1.8357e+00,  9.9503e-01,  1.2337e+00,  1.8653e-01],\n",
       "        [ 1.5066e+00, -7.3187e-01,  9.9331e-01, -7.9625e-01],\n",
       "        [ 5.4917e-01,  1.0312e+00,  1.8521e-03,  1.3660e+00],\n",
       "        [ 3.4687e+00,  8.6966e-01,  2.7240e+00,  6.3711e-01],\n",
       "        [ 3.8583e+00,  9.9451e-01,  2.8327e+00,  5.2888e-01],\n",
       "        [ 2.8155e+00,  1.9625e-02,  1.6531e+00, -3.8845e-01],\n",
       "        [ 2.0822e+00,  1.3365e+00,  4.5690e-01,  3.0951e-01],\n",
       "        [ 1.7686e+00,  4.5190e-01,  1.1205e+00,  1.0960e-01],\n",
       "        [ 3.9331e+00,  6.2073e-01,  2.8754e+00,  6.7833e-01],\n",
       "        [ 1.8394e+00,  6.3322e-01,  1.3783e+00,  3.1600e-01],\n",
       "        [ 2.7966e+00,  9.2719e-01,  2.4099e+00,  7.4733e-01],\n",
       "        [ 1.8358e+00,  3.3710e+00,  9.0747e-01,  2.9145e+00],\n",
       "        [ 2.7825e+00,  1.2857e+00,  1.4918e+00,  1.4196e+00],\n",
       "        [ 3.2516e+00,  7.1087e-01,  2.5742e+00,  6.2980e-01],\n",
       "        [ 3.9875e+00,  1.6074e+00,  2.6586e+00,  9.8108e-01],\n",
       "        [ 3.2888e+00,  1.4530e+00,  2.3090e+00,  1.3315e+00]], device='cuda:0')])>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['instances'].get_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b4a0c36",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24712/494066295.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_c2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelStudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs, branch, danchor, norm)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"instances\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for idx, inputs in enumerate(test_data_loader):\n",
    "    output = model_c2b.modelStudent(input)\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16531adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(next(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b570d71e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24712/4279426661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_all_inputs_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_c2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelTeacher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24712/4279426661.py\u001b[0m in \u001b[0;36mget_all_inputs_outputs\u001b[0;34m(data_loader, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_inputs_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs, branch, danchor, norm)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbranch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"supervised\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Region proposal network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             proposals_rpn, proposal_losses = self.proposal_generator(\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, gt_instances, compute_loss, branch, danchor)\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n\u001b[1;32m    136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_and_sample_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             losses = self.losses(\n\u001b[1;32m    139\u001b[0m                 \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_objectness_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_anchor_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mlabel_and_sample_anchors\u001b[0;34m(self, anchors, gt_instances, use_ignore, use_soft_label)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mhas_pseudo_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pseudo_boxes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mimage_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pseudo_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def get_all_inputs_outputs(data_loader,model):\n",
    "    for data in data_loader:\n",
    "        yield data, model(data)\n",
    "\n",
    "\n",
    "evaluator.reset()\n",
    "for inputs, outputs in get_all_inputs_outputs(test_data_loader,model_c2b.modelTeacher):\n",
    "      evaluator.process(inputs, outputs)\n",
    "eval_results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, inputs in enumerate(test_data_loader):\n",
    "\n",
    "    outputs = model_teacher(inputs)\n",
    "    evaluator.process(inputs, outputs)\n",
    "results = evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
