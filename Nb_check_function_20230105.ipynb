{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ccde544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/superorange5/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:803: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pt.data.dataset_mapper import DatasetMapperTwoCropSeparate\n",
    "from pt import add_config\n",
    "import pt.data.datasets.builtin\n",
    "from pt.data.build import build_detection_targetonly_loader_two_crops\n",
    "\n",
    "from pt.engine.FLtrainer import FLtrainer\n",
    "from pt.engine.trainer import PTrainer\n",
    "from pt.modeling.meta_arch.multi_teacher import MultiTSModel\n",
    "from pt.modeling.meta_arch.rcnn import GuassianGeneralizedRCNN\n",
    "from pt.modeling.backbone.vgg import build_vgg_backbone\n",
    "from pt.modeling.proposal_generator.rpn import GuassianRPN\n",
    "from pt.modeling.roi_heads.roi_heads import GuassianROIHead\n",
    "from pt.checkpoint.detection_checkpoint import DetectionTSCheckpointer\n",
    "from pt.modeling.meta_arch.ts_ensemble import EnsembleTSModel\n",
    "\n",
    "\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from Nb_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14af059",
   "metadata": {},
   "source": [
    "# A. check datamapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccb272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'VOC2007_bddvalsmall'\n",
    "cfg = setup(\"configs/pt/final_c2b.yaml\")\n",
    "mapper = DatasetMapperTwoCropSeparate(cfg, True)\n",
    "test_data_loader = build_detection_test_loader(cfg, dataset_name,mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687e00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_iter = iter(test_data_loader)\n",
    "test_data = data_loader_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abe07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266968af",
   "metadata": {},
   "source": [
    "# B. check  build_detection_targetonly_loader_two_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376f6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'VOC2007_bddvalsmall'\n",
    "cfg = setup(\"configs/pt/final_c2b.yaml\")\n",
    "mapper = DatasetMapperTwoCropSeparate(cfg, True)\n",
    "test_data_loader= build_detection_targetonly_loader_two_crops(cfg, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac2e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_iter = iter(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904c3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(data_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e689e3",
   "metadata": {},
   "source": [
    "# C. build model, load weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971121af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"configs/multi-teacher/c2b_1cls.yaml\"\n",
    "cfg = setup(cfg_path)\n",
    "\n",
    "                              \n",
    "#DetectionCheckpointer(multi_teacher_model).resume_or_load(student_model_path, resume=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6afb9",
   "metadata": {},
   "source": [
    "## C.1 build model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0517b",
   "metadata": {},
   "source": [
    "### C.1.a get specific model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad35901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matching_heuristics': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model_path = 'keep_experiments/FedPT_ck2b/avg_ck2b/pt_ck2b_AVG_round3.pth'\n",
    "\n",
    "Trainer=PTrainer\n",
    "model = Trainer.build_model(cfg)\n",
    "model_teacher = Trainer.build_model(cfg)\n",
    "ensem_ts_model = EnsembleTSModel(model_teacher, model)    \n",
    "\n",
    "DetectionCheckpointer(ensem_ts_model).resume_or_load(student_model_path, resume=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459f4f7",
   "metadata": {},
   "source": [
    "### C.1.b get model path from cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e891c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_list = cfg.MODEL.TEACHER_PATH\n",
    "student_model_path = cfg.MODEL.STUDENT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bea134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_folder = 'keep_experiments'\n",
    "\n",
    "# model_k2b_path = os.path.join(model_folder,'k2b','model_final.pth')\n",
    "# model_c2b_path = os.path.join(model_folder,'c2b','model_{0:07d}.pth'.format(11999))\n",
    "# student_model_path = 'keep_experiments/FedPT_ck2b/avg_ck2b/pt_ck2b_AVG_round3.pth'\n",
    "\n",
    "# model_path_list = [model_k2b_path,model_c2b_path]\n",
    "\n",
    "\n",
    "# build student model\n",
    "Trainer =FLtrainer\n",
    "student_model = Trainer.build_model(cfg)\n",
    "\n",
    "# build teacher model\n",
    "model_teacher_list = []                              \n",
    "for idx in range(len(model_path_list)):\n",
    "    teacher_model = Trainer.build_model(cfg)\n",
    "    model_teacher_list.append(teacher_model)\n",
    "multi_teacher_model = MultiTSModel(model_teacher_list, student_model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd5f54",
   "metadata": {},
   "source": [
    "## C.2 load weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156ea651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load teacher model:keep_experiments/k2b/model_final.pth \n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "load teacher model:keep_experiments/c2b/model_0011999.pth \n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "load student model:keep_experiments/FedPT_ck2b/avg_ck2b/pt_ck2b_AVG_round3.pth \n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    }
   ],
   "source": [
    "model_list=[]\n",
    "for model_teacher_path in model_path_list:\n",
    "    print(\"load teacher model:{} \".format(model_teacher_path))\n",
    "    model_with_weight = load_TSmodel(cfg_path, model_teacher_path)\n",
    "    model_list.append(model_with_weight)\n",
    "    \n",
    "print(\"load student model:{} \".format(student_model_path))\n",
    "student_initial_backbone = load_TSmodel(cfg_path, student_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b81862",
   "metadata": {},
   "source": [
    "## C.3 feed model weight to multi-teacher model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bf89e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "#-----load multi teacher\n",
    "for i, model in enumerate(model_list):\n",
    "    new_teacher_dict = OrderedDict()\n",
    "    \n",
    "    source_model_dict = model.modelStudent.state_dict()\n",
    "    \n",
    "    for key, value in source_model_dict.items():\n",
    "        if key in multi_teacher_model.modelTeacher[i].state_dict().keys():\n",
    "            new_teacher_dict[key] = value    \n",
    "    multi_teacher_model.modelTeacher[i].load_state_dict(new_teacher_dict)\n",
    "    \n",
    "#----------load student\n",
    "new_student_dict = OrderedDict()\n",
    "pseudo_model_dict = student_initial_backbone.modelStudent.state_dict()\n",
    "for key, value in pseudo_model_dict.items():    \n",
    "    if key in multi_teacher_model.modelStudent.state_dict().keys():\n",
    "        new_student_dict[key] = value\n",
    "multi_teacher_model.modelStudent.load_state_dict(new_student_dict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c68df",
   "metadata": {},
   "source": [
    "## C.4 check weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_teacher_model.modelTeacher[0].state_dict()['backbone.vgg_block4.0.conv1.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929f2c3",
   "metadata": {},
   "source": [
    "# D. initial trainer (include build model and load weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35723f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    }
   ],
   "source": [
    "cfg_path = \"configs/pt/final_c2b_1class.yaml\"\n",
    "cfg = setup(cfg_path)\n",
    "Trainer2 =PTrainer(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a772416",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(Trainer2._trainer._data_loader_iter)\n",
    "        # data_q and data_k from different augmentations (q:strong, k:weak)\n",
    "        # label_strong, label_weak, unlabed_strong, unlabled_weak\n",
    "label_data_q, label_data_k, unlabel_data_q, unlabel_data_k = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16f909d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_proposals_roih_unsup_k, _ = Trainer2.process_pseudo_label(\n",
    "                proposals_roih_unsup_k, \"roih\", \"all\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a74f8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data_q = Trainer2.remove_label(unlabel_data_q)\n",
    "\n",
    "unlabel_data_q = Trainer2.add_label(\n",
    "                unlabel_data_q, pesudo_proposals_roih_unsup_k\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffe44779",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_pseudo_boxes = unlabel_data_q[0]['instances'].has('pseudo_boxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1bafd0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_pseudo_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "717fdfc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "get_event_storage() has to be called inside a 'with EventStorage(...)' context!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13475/2879827448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m record_all_unlabel_data, _, _, _ = Trainer2.model(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0munlabel_data_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsupervised\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdanchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs, branch, danchor, norm)\u001b[0m\n\u001b[1;32m     83\u001b[0m             )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             _, detector_losses = self.roi_heads(\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals_rpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/roi_heads/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# apply if training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             proposals = self.label_and_sample_proposals(\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/pt/modeling/roi_heads/roi_heads.py\u001b[0m in \u001b[0;36mlabel_and_sample_proposals\u001b[0;34m(self, proposals, targets, branch)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mproposals_with_gt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_event_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         storage.put_scalar(\n\u001b[1;32m    249\u001b[0m             \u001b[0;34m\"roi_head/num_target_fg_samples_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_fg_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/ProbabilisticTeacher/detectron2/utils/events.py\u001b[0m in \u001b[0;36mget_event_storage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mThrows\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mEventStorage\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcurrently\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m     assert len(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0m_CURRENT_STORAGE_STACK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     ), \"get_event_storage() has to be called inside a 'with EventStorage(...)' context!\"\n",
      "\u001b[0;31mAssertionError\u001b[0m: get_event_storage() has to be called inside a 'with EventStorage(...)' context!"
     ]
    }
   ],
   "source": [
    "record_all_unlabel_data, _, _, _ = Trainer2.model(\n",
    "                unlabel_data_q, branch=\"unsupervised\", danchor=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c13a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WarmupMultiStepLR is deprecated! Use LRMultipilier with fvcore ParamScheduler instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load teacher model:keep_experiments/k2b/model_final.pth \n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "load teacher model:keep_experiments/c2b/model_0011999.pth \n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n",
      "load student model:keep_experiments/FedPT_ck2b/avg_ck2b/pt_ck2b_AVG_round3.pth \n",
      "-------- pretrained model loaded ---------\n",
      "-------- pretrained model loaded ---------\n"
     ]
    }
   ],
   "source": [
    "Trainer =FLtrainer(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1123e",
   "metadata": {},
   "source": [
    "## D.1 check weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145357e",
   "metadata": {},
   "source": [
    "### D.1.a check student model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fee8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0137, -0.0116, -0.0033],\n",
       "         [-0.0155, -0.0198, -0.0163],\n",
       "         [-0.0222, -0.0169, -0.0093]],\n",
       "\n",
       "        [[ 0.0027,  0.0018,  0.0166],\n",
       "         [-0.0079, -0.0036,  0.0065],\n",
       "         [ 0.0034, -0.0087, -0.0048]],\n",
       "\n",
       "        [[ 0.0173, -0.0144, -0.0179],\n",
       "         [ 0.0332,  0.0345, -0.0020],\n",
       "         [-0.0202,  0.0205,  0.0155]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0042, -0.0144,  0.0045],\n",
       "         [-0.0033, -0.0228, -0.0189],\n",
       "         [-0.0039,  0.0025,  0.0079]],\n",
       "\n",
       "        [[-0.0238, -0.0171, -0.0193],\n",
       "         [-0.0165, -0.0141,  0.0041],\n",
       "         [ 0.0024,  0.0097,  0.0248]],\n",
       "\n",
       "        [[ 0.0025, -0.0053, -0.0080],\n",
       "         [-0.0036, -0.0036, -0.0048],\n",
       "         [-0.0152, -0.0060, -0.0088]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer.model.state_dict()['backbone.vgg_block4.0.conv1.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440a159",
   "metadata": {},
   "source": [
    "### D.1.b check teacher model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61b67d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3013e-02, -1.7674e-02, -8.7242e-03],\n",
       "         [-6.6417e-03, -2.8179e-02, -1.7347e-02],\n",
       "         [-1.4393e-02, -2.5782e-02, -1.2142e-02]],\n",
       "\n",
       "        [[-7.6653e-03,  7.4842e-03,  2.7621e-02],\n",
       "         [-2.5077e-03,  9.7199e-03,  3.3551e-02],\n",
       "         [-1.8230e-02, -2.0012e-03,  1.7729e-02]],\n",
       "\n",
       "        [[-1.1370e-02, -1.1831e-02, -1.1899e-02],\n",
       "         [-7.7858e-04,  6.0030e-02,  2.9155e-02],\n",
       "         [-2.8562e-02,  2.3417e-02,  3.1388e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.1497e-03, -1.9339e-03, -4.5325e-05],\n",
       "         [ 1.7527e-04, -8.6480e-03, -2.3997e-02],\n",
       "         [ 3.6784e-03,  2.5598e-02,  1.0689e-02]],\n",
       "\n",
       "        [[-4.1432e-02, -3.3276e-02, -2.5533e-02],\n",
       "         [-1.2606e-02, -1.8507e-02, -1.0148e-02],\n",
       "         [-3.0346e-02, -7.8686e-03,  1.8107e-02]],\n",
       "\n",
       "        [[ 1.1084e-02, -2.5984e-03,  2.4725e-03],\n",
       "         [-1.0399e-02, -8.0159e-03, -3.6350e-03],\n",
       "         [-1.2528e-02, -2.8805e-03, -2.7278e-03]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer.model_teacher_list[0].state_dict()['backbone.vgg_block4.0.conv1.weight'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb45646",
   "metadata": {},
   "source": [
    "## D.2 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8aaecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data_q, unlabel_data_k  = next(Trainer._trainer._data_loader_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7130da6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'data/VOC2007_bddval/JPEGImages/b1c66a42-6f7d68ca.jpg',\n",
       " 'image_id': 'b1c66a42-6f7d68ca',\n",
       " 'height': 720,\n",
       " 'width': 1280,\n",
       " 'instances': Instances(num_instances=19, image_height=600, image_width=1067, fields=[gt_boxes: Boxes(tensor([[ 171.7203,  281.6667,  235.9070,  324.1667],\n",
       "         [  39.1789,  286.6667,  109.2008,  335.0000],\n",
       "         [ 205.8977,  286.6667,  290.0906,  330.8333],\n",
       "         [   0.0000,  280.8333,   44.1805,  336.6667],\n",
       "         [ 570.1781,  297.5000,  601.8547,  328.3333],\n",
       "         [ 588.5172,  303.3333,  612.6914,  326.6667],\n",
       "         [ 606.0226,  305.0000,  635.1984,  335.0000],\n",
       "         [ 624.3617,  301.6667,  675.2109,  341.6667],\n",
       "         [ 656.0383,  298.3333,  756.0695,  355.0000],\n",
       "         [ 733.5625,  313.3333,  801.0836,  377.5000],\n",
       "         [ 780.2438,  280.0000, 1011.1492,  404.1667],\n",
       "         [1003.6469,  346.6667, 1067.0000,  442.5000],\n",
       "         [ 460.1437,  296.6667,  471.8141,  307.5000],\n",
       "         [ 478.4828,  293.3333,  489.3195,  308.3333],\n",
       "         [ 484.3180,  297.5000,  499.3227,  315.8333],\n",
       "         [ 496.8219,  293.3333,  530.9992,  322.5000],\n",
       "         [ 431.8016,  296.6667,  446.8062,  307.5000],\n",
       "         [ 418.4641,  297.5000,  432.6352,  310.0000],\n",
       "         [ 377.6180,  294.1667,  400.1250,  311.6667]])), gt_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]),\n",
       " 'image': tensor([[[254, 255, 255,  ...,  57,  54,  51],\n",
       "          [243, 250, 253,  ...,  56,  53,  50],\n",
       "          [206, 231, 243,  ...,  54,  51,  48],\n",
       "          ...,\n",
       "          [ 49,  49,  49,  ...,  28,  28,  28],\n",
       "          [ 47,  47,  47,  ...,  26,  26,  26],\n",
       "          [ 47,  47,  47,  ...,  25,  25,  25]],\n",
       " \n",
       "         [[254, 255, 254,  ...,  89,  86,  83],\n",
       "          [241, 249, 251,  ...,  88,  85,  82],\n",
       "          [202, 227, 239,  ...,  86,  83,  80],\n",
       "          ...,\n",
       "          [ 58,  58,  58,  ...,  23,  23,  23],\n",
       "          [ 56,  56,  56,  ...,  21,  21,  21],\n",
       "          [ 56,  56,  56,  ...,  20,  20,  20]],\n",
       " \n",
       "         [[222, 219, 216,  ..., 124, 121, 118],\n",
       "          [207, 215, 217,  ..., 123, 120, 117],\n",
       "          [174, 199, 211,  ..., 121, 118, 115],\n",
       "          ...,\n",
       "          [ 61,  61,  61,  ...,  20,  20,  20],\n",
       "          [ 59,  59,  59,  ...,  18,  18,  18],\n",
       "          [ 59,  59,  59,  ...,  17,  17,  17]]], dtype=torch.uint8)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel_data_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7674990",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data_q = Trainer.remove_label(unlabel_data_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff7ad253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'data/VOC2007_bddval/JPEGImages/b1c66a42-6f7d68ca.jpg',\n",
       " 'image_id': 'b1c66a42-6f7d68ca',\n",
       " 'height': 720,\n",
       " 'width': 1280,\n",
       " 'image': tensor([[[254, 255, 255,  ...,  57,  54,  51],\n",
       "          [243, 250, 253,  ...,  56,  53,  50],\n",
       "          [206, 231, 243,  ...,  54,  51,  48],\n",
       "          ...,\n",
       "          [ 49,  49,  49,  ...,  28,  28,  28],\n",
       "          [ 47,  47,  47,  ...,  26,  26,  26],\n",
       "          [ 47,  47,  47,  ...,  25,  25,  25]],\n",
       " \n",
       "         [[254, 255, 254,  ...,  89,  86,  83],\n",
       "          [241, 249, 251,  ...,  88,  85,  82],\n",
       "          [202, 227, 239,  ...,  86,  83,  80],\n",
       "          ...,\n",
       "          [ 58,  58,  58,  ...,  23,  23,  23],\n",
       "          [ 56,  56,  56,  ...,  21,  21,  21],\n",
       "          [ 56,  56,  56,  ...,  20,  20,  20]],\n",
       " \n",
       "         [[222, 219, 216,  ..., 124, 121, 118],\n",
       "          [207, 215, 217,  ..., 123, 120, 117],\n",
       "          [174, 199, 211,  ..., 121, 118, 115],\n",
       "          ...,\n",
       "          [ 61,  61,  61,  ...,  20,  20,  20],\n",
       "          [ 59,  59,  59,  ...,  18,  18,  18],\n",
       "          [ 59,  59,  59,  ...,  17,  17,  17]]], dtype=torch.uint8)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel_data_q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "445904e0",
   "metadata": {},
   "outputs": [],
   "source": [
    " unlabel_data_q = Trainer.add_label( unlabel_data_q, list_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04427d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boxes(tensor([[ 657.4378,  299.9938,  760.4671,  359.7356],\n",
       "        [ 765.3511,  269.6281, 1029.1239,  405.7620],\n",
       "        [ 621.5120,  301.7120,  683.0803,  345.3318],\n",
       "        [  17.6943,  287.6151,  113.2596,  337.7801],\n",
       "        [ 498.5867,  296.2545,  530.5364,  319.8307],\n",
       "        [ 608.2769,  306.8873,  643.7432,  338.5808],\n",
       "        [ 569.6702,  301.0650,  601.6965,  328.6351],\n",
       "        [ 735.0723,  310.4042,  828.6488,  379.3593],\n",
       "        [ 202.1344,  288.5207,  295.6842,  333.0471],\n",
       "        [   3.5361,  282.9872,   52.3831,  341.2922],\n",
       "        [ 169.4056,  282.2881,  237.7195,  321.9904],\n",
       "        [ 585.1887,  306.8611,  622.1169,  333.6696],\n",
       "        [1002.3305,  347.5714, 1054.6060,  429.4755],\n",
       "        [ 371.9883,  294.8872,  403.8795,  312.0861],\n",
       "        [ 333.9984,  297.4119,  356.9525,  311.9879],\n",
       "        [ 177.5817,  299.2591,  246.0390,  386.8783],\n",
       "        [ 274.4435,  297.6523,  308.4064,  317.2697],\n",
       "        [ 300.6526,  297.0209,  327.9620,  313.8780],\n",
       "        [ 801.5817,  202.4449,  861.7197,  258.6296],\n",
       "        [ 309.4128,  295.1339,  344.3192,  313.6106],\n",
       "        [ 341.0116,  295.9650,  365.1990,  311.6777],\n",
       "        [ 178.8416,  284.8560,  271.4988,  327.2003],\n",
       "        [ 485.1294,  293.8854,  507.9919,  313.8454],\n",
       "        [ 577.3619,  312.0500,  598.7772,  331.0096],\n",
       "        [ 478.9757,  284.6370,  508.5193,  311.2427],\n",
       "        [ 778.3448,  297.8275,  862.2169,  386.0266],\n",
       "        [1007.8701,  356.4241, 1034.4406,  432.7000],\n",
       "        [1064.0217,    5.3168, 1066.9170,   85.0170],\n",
       "        [ 307.3831,  301.4331,  336.2183,  318.0411],\n",
       "        [ 331.8978,  300.7650,  361.8225,  315.8113],\n",
       "        [ 767.7409,  280.0699,  920.8958,  396.4110],\n",
       "        [ 508.7934,  261.4854,  536.6305,  298.2467],\n",
       "        [ 842.3151,  229.5695,  930.8202,  291.7634],\n",
       "        [ 265.2539,  299.9316,  301.1862,  322.6064],\n",
       "        [ 364.2838,  297.4492,  387.5875,  311.4073],\n",
       "        [ 349.3260,  291.1495,  377.5939,  310.5754],\n",
       "        [ 939.5334,  262.9645, 1043.4827,  395.8977],\n",
       "        [ 388.5994,  296.7073,  414.9230,  311.6600],\n",
       "        [ 537.9651,  289.1109,  567.5659,  318.3456],\n",
       "        [ 395.0706,  293.9879,  421.8651,  310.0812],\n",
       "        [ 377.6315,  291.8596,  409.0344,  308.3073],\n",
       "        [ 398.5309,  290.6240,  436.1865,  311.6025]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel_data_q[0]['instances'].pseudo_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53782f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data_q = Trainer.resize(unlabel_data_q)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de4d1061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabel_data_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f03726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    print(unlabel_data_q[i]['instances'].has('gt_boxes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ec3a0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'data/VOC2007_bddval/JPEGImages/b1c81faa-c80764c5.jpg',\n",
       " 'image_id': 'b1c81faa-c80764c5',\n",
       " 'height': 720,\n",
       " 'width': 1280,\n",
       " 'instances': Instances(num_instances=11, image_height=600, image_width=1067, fields=[gt_boxes: Boxes(tensor([[659.3727, 228.3333, 672.7101, 240.0000],\n",
       "         [628.5297, 226.6667, 649.3695, 240.8333],\n",
       "         [610.1906, 230.0000, 621.8610, 240.8333],\n",
       "         [587.6836, 231.6667, 597.6867, 240.0000],\n",
       "         [569.3445, 233.3333, 585.1828, 240.8333],\n",
       "         [459.3102, 235.0000, 471.8141, 244.1667],\n",
       "         [470.1469, 235.0000, 482.6508, 245.8333],\n",
       "         [486.8188, 232.5000, 506.8250, 244.1667],\n",
       "         [522.6633, 235.0000, 541.0023, 247.5000],\n",
       "         [527.6649, 235.0000, 556.8406, 256.6667],\n",
       "         [692.7164, 220.8333, 769.4070, 272.5000]])), gt_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]),\n",
       " 'image': tensor([[[2, 2, 2,  ..., 1, 1, 1],\n",
       "          [2, 2, 2,  ..., 1, 1, 1],\n",
       "          [2, 2, 2,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [7, 7, 7,  ..., 1, 1, 1],\n",
       "          [7, 7, 7,  ..., 1, 1, 1],\n",
       "          [7, 7, 7,  ..., 1, 1, 1]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]],\n",
       " \n",
       "         [[3, 3, 3,  ..., 1, 1, 1],\n",
       "          [3, 3, 3,  ..., 1, 1, 1],\n",
       "          [3, 3, 3,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [2, 2, 2,  ..., 1, 1, 1],\n",
       "          [2, 2, 2,  ..., 1, 1, 1],\n",
       "          [2, 2, 2,  ..., 1, 1, 1]]], dtype=torch.uint8)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel_data_q[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9568bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "( _, proposals_rpn_unsup_k, proposals_roih_unsup_k, _,) = Trainer.model_teacher_list[0](unlabel_data_k, branch=\"unsup_data_weak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abf8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "( _, proposals_rpn_unsup_k2, proposals_roih_unsup_k2, _,) = Trainer.model_teacher_list[1](unlabel_data_k, branch=\"unsup_data_weak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a544467b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proposals_roih_unsup_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45cb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_match_array_nogt(proposals_roih):\n",
    "    source_num = len(proposals_roih)\n",
    "    batch_size = len(proposals_roih[0])\n",
    "    \n",
    "    batch_match_array= []\n",
    "    for data_idx in range(batch_size):\n",
    "\n",
    "        #pairwise_src2others = []\n",
    "        match_array_source = []\n",
    "        for i, source_prediction_n in enumerate(proposals_roih):\n",
    "            match_array_source_n =[]\n",
    "            # others\n",
    "            #pairwise_sa_sb = []        \n",
    "            for j in range(source_num):\n",
    "                if j!=i:\n",
    "\n",
    "                    sourcen_n_match_other = structures.pairwise_iou(source_prediction_n[data_idx].get('pred_boxes'),proposals_roih[j][data_idx].get('pred_boxes'))\n",
    "                    #pairwise_sa_sb.append(sourcen_n_match_other)\n",
    "                    match_array_source_n.append(get_match_array(sourcen_n_match_other))\n",
    "            match_array_source.append(match_array_source_n)  \n",
    "        batch_match_array.append(match_array_source)\n",
    "                                                                   \n",
    "    return  batch_match_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd0750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_proposals_roih = [proposals_roih_unsup_k,proposals_roih_unsup_k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46cc4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mt_src = get_match_array_nogt(multi_proposals_roih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d2eecd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_mt_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb33e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_ensemble(mt_src,src_idx):\n",
    "    source_num = len(mt_src[0])\n",
    "    df_src = pd.DataFrame()    \n",
    "    src_array = np.array(mt_src[src_idx]).T\n",
    "    df_src = pd.DataFrame(src_array)\n",
    "    df_src['summary'] = df_src.sum(axis=1)\n",
    "    keep_index = df_src.index[df_src.summary==source_num]\n",
    "    return keep_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d87b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_idx = 0\n",
    "batch_size=16\n",
    "pesudo_proposals_roih_combined = []\n",
    "for batch_idx in range(batch_size):\n",
    "    keep_index = bb_ensemble(batch_mt_src[batch_idx],src_idx)\n",
    "    pesudo_proposals_roih_combined.append( multi_proposals_roih[src_idx][batch_idx][keep_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bc65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pt.structures.instances import FreeInstances\n",
    "\n",
    "def process_pseudo_label2(proposals_roih):\n",
    "    list_instances = []\n",
    "    # change to 1 dim temporary\n",
    "    for proposal_bbox_inst in proposals_roih:\n",
    "\n",
    "        image_shape = proposal_bbox_inst.image_size\n",
    "        new_proposal_inst = FreeInstances(image_shape)\n",
    "\n",
    "        new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor\n",
    "        pseudo_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "                # add boxes to instances\n",
    "        new_proposal_inst.pseudo_boxes = pseudo_boxes\n",
    "        new_proposal_inst.scores_logists = proposal_bbox_inst.scores_logists\n",
    "        if proposal_bbox_inst.has('boxes_sigma'):\n",
    "            new_proposal_inst.boxes_sigma = proposal_bbox_inst.boxes_sigma\n",
    "        list_instances.append(new_proposal_inst)\n",
    "    return list_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f59a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_instances = process_pseudo_label2(pesudo_proposals_roih_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b890a774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instances(num_instances=8, image_height=600, image_width=1067, fields=[pred_boxes: Boxes(tensor([[694.5236, 220.3513, 776.3004, 275.6724],\n",
       "        [528.8289, 234.4392, 560.0524, 257.0865],\n",
       "        [572.5876, 135.3076, 651.8466, 188.6729],\n",
       "        [499.4250, 131.8156, 587.2648, 192.0158],\n",
       "        [593.0430, 214.1681, 661.5519, 243.5728],\n",
       "        [567.9817, 223.9365, 606.1909, 247.8848],\n",
       "        [460.5973, 231.8435, 491.3189, 250.8547],\n",
       "        [621.9167, 215.0542, 672.1984, 242.8776]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>)), scores: tensor([0.9704, 0.9069, 0.8632, 0.8576, 0.0203, 0.0184, 0.0147, 0.0101],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), scores_logists: tensor([[ 3.5231, -3.5369],\n",
       "        [ 2.7387, -2.7303],\n",
       "        [ 1.6161, -1.6111],\n",
       "        [ 2.0313, -1.9718],\n",
       "        [-1.2048,  1.2886],\n",
       "        [-1.3112,  1.3979],\n",
       "        [-1.4033,  1.4871],\n",
       "        [-1.3437,  1.4549]], device='cuda:0', grad_fn=<IndexBackward>), boxes_sigma: tensor([[-3.7711, -3.7226, -3.2667, -3.4059],\n",
       "        [-2.7266, -2.3085, -2.1975, -2.1389],\n",
       "        [-2.4679, -2.4854, -1.8964, -1.9502],\n",
       "        [-2.2456, -2.4115, -1.8825, -1.4059],\n",
       "        [ 2.2700,  0.0755,  2.0862,  0.4935],\n",
       "        [ 1.1119,  0.4027,  1.2864,  0.7869],\n",
       "        [ 1.9544,  0.1903,  2.0337,  0.3047],\n",
       "        [ 2.1631,  0.9270,  2.1810,  1.2736]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roih_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3712c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pt.structures.instances import FreeInstances\n",
    "list_instances = []\n",
    "#or proposal_bbox_inst in roih_combined:\n",
    "proposal_bbox_inst =  roih_combined\n",
    "image_shape = proposal_bbox_inst.image_size\n",
    "new_proposal_inst = FreeInstances(image_shape)\n",
    "\n",
    "new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor\n",
    "pseudo_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "        # add boxes to instances\n",
    "new_proposal_inst.pseudo_boxes = pseudo_boxes\n",
    "new_proposal_inst.scores_logists = proposal_bbox_inst.scores_logists\n",
    "if proposal_bbox_inst.has('boxes_sigma'):\n",
    "    new_proposal_inst.boxes_sigma = proposal_bbox_inst.boxes_sigma\n",
    "list_instances.append(new_proposal_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19077860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreeInstances(num_instances=8, image_height=600, image_width=1067, fields=[pseudo_boxes: Boxes(tensor([[694.5236, 220.3513, 776.3004, 275.6724],\n",
       "         [528.8289, 234.4392, 560.0524, 257.0865],\n",
       "         [572.5876, 135.3076, 651.8466, 188.6729],\n",
       "         [499.4250, 131.8156, 587.2648, 192.0158],\n",
       "         [593.0430, 214.1681, 661.5519, 243.5728],\n",
       "         [567.9817, 223.9365, 606.1909, 247.8848],\n",
       "         [460.5973, 231.8435, 491.3189, 250.8547],\n",
       "         [621.9167, 215.0542, 672.1984, 242.8776]], device='cuda:0',\n",
       "        grad_fn=<IndexBackward>)), scores_logists: tensor([[ 3.5231, -3.5369],\n",
       "         [ 2.7387, -2.7303],\n",
       "         [ 1.6161, -1.6111],\n",
       "         [ 2.0313, -1.9718],\n",
       "         [-1.2048,  1.2886],\n",
       "         [-1.3112,  1.3979],\n",
       "         [-1.4033,  1.4871],\n",
       "         [-1.3437,  1.4549]], device='cuda:0', grad_fn=<IndexBackward>), boxes_sigma: tensor([[-3.7711, -3.7226, -3.2667, -3.4059],\n",
       "         [-2.7266, -2.3085, -2.1975, -2.1389],\n",
       "         [-2.4679, -2.4854, -1.8964, -1.9502],\n",
       "         [-2.2456, -2.4115, -1.8825, -1.4059],\n",
       "         [ 2.2700,  0.0755,  2.0862,  0.4935],\n",
       "         [ 1.1119,  0.4027,  1.2864,  0.7869],\n",
       "         [ 1.9544,  0.1903,  2.0337,  0.3047],\n",
       "         [ 2.1631,  0.9270,  2.1810,  1.2736]], device='cuda:0',\n",
       "        grad_fn=<IndexBackward>)])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4500dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pseudo_label(\n",
    "            self, proposals_rpn_unsup_k, proposal_type, psedo_label_method=\"\"\n",
    "    ):\n",
    "        list_instances = []\n",
    "        num_proposal_output = 0.0\n",
    "        for proposal_bbox_inst in proposals_rpn_unsup_k:\n",
    "            # all\n",
    "            if psedo_label_method == \"all\":\n",
    "                proposal_bbox_inst = self.threshold_bbox(\n",
    "                    proposal_bbox_inst, proposal_type=proposal_type\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Unkown pseudo label boxes methods\")\n",
    "            num_proposal_output += len(proposal_bbox_inst)\n",
    "            list_instances.append(proposal_bbox_inst)\n",
    "        num_proposal_output = num_proposal_output / len(proposals_rpn_unsup_k)\n",
    "        return list_instances, num_proposal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "input proposal_bbox_inst\n",
    "            image_shape = proposal_bbox_inst.image_size\n",
    "            new_proposal_inst = FreeInstances(image_shape)\n",
    "\n",
    "            # create box\n",
    "            # new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor\n",
    "            # new_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "            # add boxes to instances\n",
    "            # new_proposal_inst.gt_boxes = new_boxes\n",
    "            # new_proposal_inst.gt_classes = proposal_bbox_inst.pred_classes\n",
    "            # new_proposal_inst.scores = proposal_bbox_inst.scores\n",
    "\n",
    "            # ------------ <all -----------\n",
    "            new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor\n",
    "            pseudo_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "            # add boxes to instances\n",
    "            new_proposal_inst.pseudo_boxes = pseudo_boxes\n",
    "            new_proposal_inst.scores_logists = proposal_bbox_inst.scores_logists\n",
    "            if proposal_bbox_inst.has('boxes_sigma'):\n",
    "                new_proposal_inst.boxes_sigma = proposal_bbox_inst.boxes_sigma\n",
    "\n",
    "        return new_proposal_inst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b61c70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreeInstances(num_instances=43, image_height=600, image_width=1067, fields=[pred_boxes: Boxes(tensor([[3.1803e+02, 3.0278e+02, 4.0796e+02, 3.5781e+02],\n",
       "        [4.1591e+01, 2.8276e+02, 2.9419e+02, 4.0292e+02],\n",
       "        [9.6065e+02, 2.9093e+02, 1.0332e+03, 3.3434e+02],\n",
       "        [3.8601e+02, 3.0232e+02, 4.4282e+02, 3.4395e+02],\n",
       "        [3.8055e-02, 3.5004e+02, 6.5678e+01, 4.3995e+02],\n",
       "        [2.6332e+02, 3.1382e+02, 3.3319e+02, 3.7734e+02],\n",
       "        [5.3871e+02, 2.9721e+02, 5.6825e+02, 3.2123e+02],\n",
       "        [7.7827e+02, 2.9077e+02, 8.6378e+02, 3.3127e+02],\n",
       "        [8.4443e+02, 2.8528e+02, 8.9837e+02, 3.1929e+02],\n",
       "        [4.2582e+02, 3.0485e+02, 4.5922e+02, 3.3758e+02],\n",
       "        [4.5356e+02, 3.0418e+02, 4.8237e+02, 3.3065e+02],\n",
       "        [4.6781e+02, 2.9858e+02, 4.9838e+02, 3.2622e+02],\n",
       "        [7.5422e+02, 2.9465e+02, 7.8948e+02, 3.1545e+02],\n",
       "        [4.4647e+02, 3.0575e+02, 4.7286e+02, 3.3301e+02],\n",
       "        [8.2748e+02, 3.1024e+02, 8.9089e+02, 3.7915e+02],\n",
       "        [7.2724e+02, 2.9507e+02, 7.5307e+02, 3.1209e+02],\n",
       "        [7.4400e+02, 2.9433e+02, 7.7434e+02, 3.1464e+02],\n",
       "        [5.6007e+02, 2.7965e+02, 5.9946e+02, 3.1201e+02],\n",
       "        [1.0244e+03, 2.8243e+02, 1.0670e+03, 3.3808e+02],\n",
       "        [1.2749e+00, 3.5035e+02, 3.1439e+01, 4.3938e+02],\n",
       "        [7.2132e+02, 2.9501e+02, 7.4436e+02, 3.1002e+02],\n",
       "        [7.5924e+02, 2.9161e+02, 8.0185e+02, 3.1883e+02],\n",
       "        [7.5139e+02, 2.8698e+02, 8.8976e+02, 3.7169e+02],\n",
       "        [4.8726e+02, 2.9891e+02, 5.1083e+02, 3.2327e+02],\n",
       "        [5.8621e+02, 3.0290e+02, 6.2146e+02, 3.2574e+02],\n",
       "        [5.0308e+02, 3.0177e+02, 5.3171e+02, 3.2313e+02],\n",
       "        [8.7757e+02, 2.5466e+02, 9.4309e+02, 3.0831e+02],\n",
       "        [5.5088e+02, 2.9351e+02, 5.7656e+02, 3.1770e+02],\n",
       "        [5.8472e+02, 3.1060e+02, 6.1589e+02, 3.5159e+02],\n",
       "        [5.1980e+02, 2.9972e+02, 5.4304e+02, 3.2016e+02],\n",
       "        [5.2777e+02, 2.9767e+02, 5.4983e+02, 3.1821e+02],\n",
       "        [1.0623e+03, 6.4676e+01, 1.0670e+03, 1.2152e+02],\n",
       "        [8.9133e+02, 2.8705e+02, 9.4052e+02, 3.1429e+02],\n",
       "        [7.7455e+02, 2.9045e+02, 8.2837e+02, 3.2259e+02],\n",
       "        [5.0884e+02, 2.9782e+02, 5.3713e+02, 3.1923e+02],\n",
       "        [1.0617e+03, 1.0709e+02, 1.0670e+03, 1.6462e+02],\n",
       "        [7.0112e+02, 2.9360e+02, 7.3031e+02, 3.1133e+02],\n",
       "        [6.8186e+02, 2.9111e+02, 7.3282e+02, 3.1316e+02],\n",
       "        [6.9235e+02, 2.9165e+02, 7.1954e+02, 3.0824e+02],\n",
       "        [5.1829e+02, 2.9202e+02, 5.4600e+02, 3.1384e+02],\n",
       "        [5.5860e+02, 2.9001e+02, 5.8572e+02, 3.1527e+02],\n",
       "        [6.5603e+02, 2.9868e+02, 6.9179e+02, 3.2085e+02],\n",
       "        [6.8630e+02, 2.9515e+02, 7.1245e+02, 3.1241e+02]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>)), scores: tensor([0.9783, 0.9781, 0.9592, 0.9570, 0.9489, 0.9413, 0.8890, 0.8494, 0.7300,\n",
       "        0.6850, 0.6181, 0.5981, 0.5054, 0.4719, 0.4170, 0.2578, 0.2281, 0.1591,\n",
       "        0.1121, 0.1025, 0.0917, 0.0466, 0.0465, 0.0425, 0.0376, 0.0344, 0.0338,\n",
       "        0.0330, 0.0318, 0.0315, 0.0246, 0.0237, 0.0217, 0.0201, 0.0194, 0.0175,\n",
       "        0.0142, 0.0142, 0.0117, 0.0113, 0.0110, 0.0100, 0.0095],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0'), scores_logists: tensor([[ 3.9452, -3.9680],\n",
       "        [ 3.4069, -3.4559],\n",
       "        [ 3.6061, -3.6082],\n",
       "        [ 3.3734, -3.3885],\n",
       "        [ 2.9452, -2.9920],\n",
       "        [ 2.7652, -2.8136],\n",
       "        [ 1.8362, -1.8162],\n",
       "        [ 2.2421, -2.2937],\n",
       "        [ 1.1392, -1.1617],\n",
       "        [ 1.1270, -1.0453],\n",
       "        [ 2.4507, -2.4066],\n",
       "        [ 1.2369, -1.1501],\n",
       "        [ 1.3352, -1.3423],\n",
       "        [ 1.5892, -1.5651],\n",
       "        [ 0.2150, -0.1625],\n",
       "        [ 0.3445, -0.2996],\n",
       "        [ 0.2511, -0.1956],\n",
       "        [ 0.1505, -0.0958],\n",
       "        [-0.4685,  0.4762],\n",
       "        [-0.6410,  0.6585],\n",
       "        [-0.3833,  0.3834],\n",
       "        [-0.8885,  0.9884],\n",
       "        [-1.2698,  1.2459],\n",
       "        [-0.8992,  0.9704],\n",
       "        [-1.1996,  1.1826],\n",
       "        [-0.9959,  1.0263],\n",
       "        [-1.1255,  1.1530],\n",
       "        [-1.0386,  1.0234],\n",
       "        [-1.3296,  1.2900],\n",
       "        [-1.1888,  1.2018],\n",
       "        [-1.3401,  1.3519],\n",
       "        [-1.0691,  1.0937],\n",
       "        [-1.2173,  1.2311],\n",
       "        [-1.3914,  1.3851],\n",
       "        [-1.1675,  1.1516],\n",
       "        [-1.2257,  1.2385],\n",
       "        [-1.1858,  1.2010],\n",
       "        [-1.1975,  1.2051],\n",
       "        [-1.0425,  1.0567],\n",
       "        [-1.4561,  1.4464],\n",
       "        [-1.0272,  1.0821],\n",
       "        [-1.3602,  1.3714],\n",
       "        [-1.2232,  1.2228]], device='cuda:0', grad_fn=<IndexBackward>), boxes_sigma: tensor([[-3.8084, -4.0059, -3.6137, -3.9168],\n",
       "        [-3.7868, -4.2303, -3.5418, -3.9426],\n",
       "        [-3.1569, -3.3525, -3.0003, -3.2205],\n",
       "        [-3.0998, -3.1956, -2.8713, -3.4187],\n",
       "        [-3.2008, -2.9480, -2.9607, -2.8187],\n",
       "        [-2.8721, -3.0199, -2.5216, -3.0333],\n",
       "        [-2.3550, -2.3308, -2.2232, -2.4598],\n",
       "        [-1.5424, -2.4243, -1.5636, -1.8552],\n",
       "        [-1.4631, -1.9356, -1.0455, -1.3049],\n",
       "        [-0.5161, -1.8826, -0.7510, -1.9792],\n",
       "        [ 0.8967, -0.9806, -0.4815, -1.7853],\n",
       "        [ 0.3293, -1.4231, -0.1440, -1.7507],\n",
       "        [ 0.7059, -0.6723,  0.2025, -0.9367],\n",
       "        [ 2.3328, -0.8845,  0.6223, -1.5347],\n",
       "        [-1.8584, -0.6567, -1.1352, -0.1257],\n",
       "        [ 2.6184, -0.6496,  1.8191, -0.8883],\n",
       "        [ 3.4658, -0.6352,  2.1713, -0.8954],\n",
       "        [ 1.6440,  0.8588,  1.2495,  0.1943],\n",
       "        [ 2.2625, -0.8968,  2.1605, -0.8177],\n",
       "        [ 2.7281, -1.7760,  1.5037, -1.4793],\n",
       "        [ 2.4294,  0.4562,  1.6892, -0.1252],\n",
       "        [ 2.6326, -0.2614,  1.7660, -0.5198],\n",
       "        [-0.6950, -0.8077, -0.4209, -0.1009],\n",
       "        [ 2.5836,  0.0817,  1.5882, -0.2141],\n",
       "        [-0.0187,  0.6089, -0.0077,  0.3203],\n",
       "        [ 1.6393,  0.7175,  1.0217,  0.3175],\n",
       "        [ 0.1991,  1.0175,  0.2467,  0.8504],\n",
       "        [ 3.0888,  0.7434,  1.3993, -0.4252],\n",
       "        [-0.1464,  0.6792, -0.1954,  0.1819],\n",
       "        [ 1.1759,  0.3490,  0.6581, -0.0395],\n",
       "        [ 2.0379, -0.0563,  0.9851, -0.6040],\n",
       "        [ 2.1976,  0.9473,  1.5881,  0.5334],\n",
       "        [ 0.7235,  1.3851,  0.4336,  1.5953],\n",
       "        [ 2.5628, -0.2012,  1.7285, -0.4067],\n",
       "        [ 2.2538,  1.2320,  1.3713,  0.6470],\n",
       "        [ 2.4788,  0.9738,  1.5872,  0.5352],\n",
       "        [ 4.5625,  0.9685,  3.4723,  0.5862],\n",
       "        [ 3.8787,  0.7748,  3.0362,  0.8431],\n",
       "        [ 2.9386,  2.2962,  2.1202,  1.5084],\n",
       "        [ 2.7873,  1.0803,  1.6591,  0.4293],\n",
       "        [ 4.3706,  2.6694,  2.6042,  1.0520],\n",
       "        [ 2.2047,  1.3498,  1.9508,  1.2271],\n",
       "        [ 4.0195,  1.5844,  3.2610,  1.0969]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals_roih_unsup_k2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76966486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreeInstances(num_instances=60, image_height=600, image_width=1067, fields=[pred_boxes: Boxes(tensor([[ 305.9895,  299.1415,  412.1760,  361.5930],\n",
       "        [ 382.3076,  300.8235,  446.7187,  345.1768],\n",
       "        [  35.6888,  270.6293,  310.1724,  405.2239],\n",
       "        [ 960.8623,  289.7416, 1049.4180,  334.9900],\n",
       "        [ 538.6613,  294.8270,  573.3475,  320.8658],\n",
       "        [ 247.0082,  311.1803,  335.0538,  380.8680],\n",
       "        [ 419.8506,  305.1220,  461.5975,  338.2710],\n",
       "        [ 463.0506,  299.4230,  500.0391,  328.9950],\n",
       "        [ 775.6105,  289.7632,  868.5215,  332.7422],\n",
       "        [   3.6932,  334.4381,   69.2010,  445.4673],\n",
       "        [ 834.1901,  284.1828,  901.1633,  322.0711],\n",
       "        [ 446.0519,  305.9645,  483.6690,  332.4266],\n",
       "        [ 823.1245,  301.1689,  893.0054,  380.1613],\n",
       "        [ 209.9434,  207.4693,  269.9500,  256.6091],\n",
       "        [ 560.9183,  286.0779,  587.5471,  311.5070],\n",
       "        [1065.3970,   15.1283, 1066.7642,  103.0660],\n",
       "        [1065.5072,    0.0000, 1065.8800,   72.7816],\n",
       "        [ 751.8559,  294.8752,  786.8796,  313.0029],\n",
       "        [ 705.7114,  296.2064,  731.1996,  312.6601],\n",
       "        [ 723.2750,  296.9673,  750.3162,  312.9613],\n",
       "        [ 569.1852,  289.5773,  590.9440,  309.5612],\n",
       "        [ 553.7021,  292.3439,  580.5784,  316.0980],\n",
       "        [ 353.3076,  300.7947,  426.3540,  351.9412],\n",
       "        [1063.4780,   85.0424, 1065.4084,  172.5897],\n",
       "        [   3.2947,  237.0519,   76.3974,  465.3050],\n",
       "        [ 502.7473,  191.9824,  544.3093,  250.3661],\n",
       "        [ 598.3542,  292.0240,  622.0894,  308.8408],\n",
       "        [1012.2874,  287.1404, 1066.5391,  336.0305],\n",
       "        [ 769.3404,  288.3787,  892.9772,  382.5323],\n",
       "        [ 746.6588,  298.0595,  771.8195,  314.6841],\n",
       "        [ 758.2357,  292.6972,  798.5984,  316.6708],\n",
       "        [ 831.4361,  136.7778,  886.6168,  194.3636],\n",
       "        [1015.1843,  294.2881, 1030.8656,  344.0129],\n",
       "        [ 738.9982,  293.9301,  773.3189,  311.5683],\n",
       "        [ 723.1044,  290.6462,  769.2912,  313.4246],\n",
       "        [ 209.7000,  295.5561,  323.9493,  391.8253],\n",
       "        [ 486.9494,  299.7585,  516.2610,  322.2315],\n",
       "        [   5.1897,  194.6694,  129.8312,  441.2789],\n",
       "        [  14.2553,  227.9985,  192.7883,  333.1063],\n",
       "        [ 563.5522,  275.9112,  593.4134,  305.1438],\n",
       "        [ 619.5044,  293.0546,  650.0461,  310.5860],\n",
       "        [1054.5984,  527.0100, 1055.8208,  589.5863],\n",
       "        [ 609.4485,  293.6010,  632.5712,  309.9174],\n",
       "        [1047.1993,  529.9131, 1048.3890,  591.5086],\n",
       "        [1049.6107,  557.9276, 1050.9364,  597.7689],\n",
       "        [ 554.8636,  281.6770,  581.6637,  307.7673],\n",
       "        [1051.1282,  571.4702, 1059.1333,  598.8361],\n",
       "        [1048.8749,  521.9045, 1050.3114,  579.7236],\n",
       "        [1051.8444,  446.8093, 1055.4938,  516.0139],\n",
       "        [ 722.3795,  286.5157,  752.2208,  307.1461],\n",
       "        [ 581.8963,  291.5362,  601.9516,  308.8050],\n",
       "        [ 697.8351,  290.9161,  726.6750,  309.6820],\n",
       "        [ 662.2612,  297.1648,  690.1985,  315.8153],\n",
       "        [1062.2698,  122.4910, 1064.0505,  191.9924],\n",
       "        [1058.5840,  122.9236, 1060.9536,  193.8161],\n",
       "        [ 589.0501,  290.6850,  611.5392,  307.0410],\n",
       "        [1049.9810,  130.7209, 1052.2058,  205.0163],\n",
       "        [1049.5626,  114.9041, 1051.7831,  189.3056],\n",
       "        [ 663.6580,  291.5857,  693.6097,  310.2198],\n",
       "        [1056.2197,  109.7155, 1058.9177,  179.3481]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>)), scores: tensor([0.9776, 0.9659, 0.9644, 0.9566, 0.9431, 0.9401, 0.8915, 0.8769, 0.8677,\n",
       "        0.8362, 0.8262, 0.7012, 0.6108, 0.4612, 0.4410, 0.2891, 0.2891, 0.1896,\n",
       "        0.1653, 0.1367, 0.1129, 0.1126, 0.0989, 0.0801, 0.0790, 0.0790, 0.0737,\n",
       "        0.0716, 0.0699, 0.0695, 0.0689, 0.0443, 0.0408, 0.0371, 0.0285, 0.0266,\n",
       "        0.0246, 0.0194, 0.0177, 0.0154, 0.0125, 0.0108, 0.0104, 0.0103, 0.0097,\n",
       "        0.0089, 0.0083, 0.0079, 0.0067, 0.0064, 0.0063, 0.0059, 0.0055, 0.0055,\n",
       "        0.0054, 0.0053, 0.0052, 0.0043, 0.0040, 0.0031], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), scores_logists: tensor([[ 3.8003, -3.8563],\n",
       "        [ 3.7125, -3.7097],\n",
       "        [ 3.3325, -3.3522],\n",
       "        [ 3.3238, -3.3128],\n",
       "        [ 3.9334, -3.9361],\n",
       "        [ 3.4539, -3.4433],\n",
       "        [ 3.0688, -3.0726],\n",
       "        [ 2.0862, -2.0377],\n",
       "        [ 2.3871, -2.3875],\n",
       "        [ 1.8364, -1.8430],\n",
       "        [ 2.4166, -2.4710],\n",
       "        [ 1.3844, -1.3632],\n",
       "        [ 0.6490, -0.6968],\n",
       "        [ 0.2115, -0.1924],\n",
       "        [ 1.0260, -0.9571],\n",
       "        [-0.0699,  0.0636],\n",
       "        [-0.0699,  0.0636],\n",
       "        [ 0.0795, -0.0328],\n",
       "        [ 0.1374, -0.0991],\n",
       "        [ 0.2935, -0.2635],\n",
       "        [ 0.1077, -0.0155],\n",
       "        [-0.1411,  0.1881],\n",
       "        [-0.8080,  0.7634],\n",
       "        [-0.2595,  0.2919],\n",
       "        [-0.6460,  0.7180],\n",
       "        [-1.0008,  1.0105],\n",
       "        [-0.1500,  0.2205],\n",
       "        [-0.7369,  0.7759],\n",
       "        [-0.8453,  0.8820],\n",
       "        [-0.4092,  0.4504],\n",
       "        [-0.8297,  0.8856],\n",
       "        [-1.3007,  1.3112],\n",
       "        [-1.2747,  1.3277],\n",
       "        [-0.6283,  0.6812],\n",
       "        [-0.8621,  0.9232],\n",
       "        [-1.4537,  1.4788],\n",
       "        [-1.1837,  1.2924],\n",
       "        [-1.1634,  1.1830],\n",
       "        [-1.2153,  1.2351],\n",
       "        [-1.2544,  1.2995],\n",
       "        [-1.0682,  1.1547],\n",
       "        [-1.1074,  1.1749],\n",
       "        [-1.0607,  1.1254],\n",
       "        [-1.1397,  1.2091],\n",
       "        [-1.2080,  1.2820],\n",
       "        [-1.3436,  1.4162],\n",
       "        [-1.2830,  1.3590],\n",
       "        [-1.2704,  1.3308],\n",
       "        [-1.4294,  1.4809],\n",
       "        [-1.2456,  1.3009],\n",
       "        [-1.2952,  1.3682],\n",
       "        [-1.2337,  1.3149],\n",
       "        [-1.3644,  1.4795],\n",
       "        [-1.3105,  1.3759],\n",
       "        [-1.3155,  1.3804],\n",
       "        [-1.2730,  1.3422],\n",
       "        [-1.3738,  1.4394],\n",
       "        [-1.3547,  1.4232],\n",
       "        [-1.3369,  1.4576],\n",
       "        [-1.3971,  1.4652]], device='cuda:0', grad_fn=<IndexBackward>), boxes_sigma: tensor([[-3.8610e+00, -3.9988e+00, -3.5358e+00, -3.8469e+00],\n",
       "        [-3.3827e+00, -3.7785e+00, -2.9508e+00, -3.5093e+00],\n",
       "        [-3.4106e+00, -3.9493e+00, -2.7290e+00, -3.6408e+00],\n",
       "        [-3.1398e+00, -3.6120e+00, -2.6571e+00, -3.3062e+00],\n",
       "        [-3.0251e+00, -2.8348e+00, -2.7095e+00, -2.7145e+00],\n",
       "        [-2.5727e+00, -3.6886e+00, -2.1690e+00, -3.2309e+00],\n",
       "        [-2.0892e+00, -2.4167e+00, -1.7994e+00, -2.2870e+00],\n",
       "        [-2.0898e+00, -2.6970e+00, -1.6359e+00, -2.2021e+00],\n",
       "        [-1.8364e+00, -2.2101e+00, -1.7938e+00, -1.9835e+00],\n",
       "        [-2.4027e+00, -1.6221e+00, -2.1567e+00, -1.2714e+00],\n",
       "        [-1.7069e+00, -2.1804e+00, -9.7634e-01, -1.8010e+00],\n",
       "        [ 3.2249e-02, -1.7593e+00, -1.2017e+00, -1.9085e+00],\n",
       "        [-2.5097e+00, -6.6362e-01, -2.0987e+00, -4.2094e-01],\n",
       "        [-1.9876e+00, -1.3055e+00, -1.1797e+00, -5.9987e-01],\n",
       "        [-3.8261e-02,  3.9744e-01, -3.8912e-01,  2.3304e-03],\n",
       "        [ 9.5551e-02, -1.0697e+00, -2.4318e-01, -8.3121e-01],\n",
       "        [ 9.5551e-02, -1.0697e+00, -2.4318e-01, -8.3121e-01],\n",
       "        [ 2.0784e+00, -4.2858e-01,  1.2223e+00,  3.1401e-02],\n",
       "        [ 2.9110e+00, -1.3130e-02,  1.9327e+00, -5.6918e-03],\n",
       "        [ 3.8236e+00,  6.5406e-01,  2.4728e+00,  3.2828e-01],\n",
       "        [ 2.2801e+00,  1.5936e+00,  5.9965e-01,  1.1807e+00],\n",
       "        [ 5.6276e+00,  6.2193e-01,  1.6543e+00, -2.5262e-01],\n",
       "        [ 1.9591e+00, -1.9137e+00,  2.2530e-01, -1.8270e+00],\n",
       "        [ 1.7096e+00,  1.4536e+00,  1.4100e+00,  6.7524e-01],\n",
       "        [-2.1269e-01,  7.2866e-01, -6.6930e-02,  1.6766e+00],\n",
       "        [-1.5273e+00, -6.7621e-01, -9.8010e-01,  1.3850e-01],\n",
       "        [ 1.6487e+00,  1.5155e+00,  1.7330e+00,  1.2048e+00],\n",
       "        [ 2.9063e+00, -1.1159e+00,  2.1584e+00, -7.4897e-01],\n",
       "        [-6.8163e-01,  3.6857e-01, -9.2949e-02,  1.0650e+00],\n",
       "        [ 2.9993e+00,  5.2819e-01,  1.8088e+00,  5.0638e-01],\n",
       "        [ 9.0887e-01, -4.0956e-01,  4.6019e-01, -1.2998e-01],\n",
       "        [-1.3349e+00, -6.4024e-01, -8.1612e-01,  2.0432e-01],\n",
       "        [ 3.5741e-01, -1.2163e+00, -1.4020e-01, -6.1404e-01],\n",
       "        [ 4.1796e+00,  8.7436e-01,  2.5060e+00,  7.8859e-01],\n",
       "        [ 3.0105e+00,  6.1199e-01,  2.3607e+00,  8.0553e-01],\n",
       "        [ 1.3767e+00, -2.0982e+00,  1.0253e+00, -1.1013e+00],\n",
       "        [ 3.5611e+00,  1.5244e-01,  1.1592e+00, -1.6477e-01],\n",
       "        [ 3.3937e+00,  4.0437e-02,  2.7928e+00,  8.1785e-01],\n",
       "        [ 5.3862e-01,  2.0986e+00,  6.1046e-01,  2.7485e+00],\n",
       "        [ 1.2269e+00,  2.1040e+00,  5.9122e-01,  1.6482e+00],\n",
       "        [ 2.9830e+00,  1.3940e+00,  2.4252e+00,  1.4823e+00],\n",
       "        [ 5.3788e+00,  2.6481e+00,  4.2158e+00,  4.9167e-01],\n",
       "        [ 3.5385e+00,  1.7296e+00,  2.7216e+00,  1.5640e+00],\n",
       "        [ 5.4608e+00,  2.6104e+00,  4.2339e+00,  4.7097e-01],\n",
       "        [ 5.8056e+00,  2.3583e+00,  4.1696e+00,  4.0459e-01],\n",
       "        [ 2.7820e+00,  2.9353e+00,  1.0621e+00,  1.1850e+00],\n",
       "        [ 5.2907e+00,  2.2760e+00,  3.7640e+00,  4.9114e-01],\n",
       "        [ 5.3695e+00,  2.5808e+00,  4.3660e+00,  5.4016e-01],\n",
       "        [ 5.1193e+00,  2.1119e+00,  4.0026e+00,  4.6456e-01],\n",
       "        [ 2.5080e+00,  2.6578e+00,  2.2094e+00,  2.0723e+00],\n",
       "        [ 5.0431e+00,  2.0206e+00,  2.5001e+00,  1.4724e+00],\n",
       "        [ 2.5991e+00,  2.5654e+00,  2.2712e+00,  2.3286e+00],\n",
       "        [ 2.6522e+00,  2.0479e+00,  2.5013e+00,  1.7540e+00],\n",
       "        [ 6.7775e+00,  3.0384e+00,  5.2672e+00,  8.9297e-01],\n",
       "        [ 6.7755e+00,  3.0279e+00,  5.2742e+00,  8.8518e-01],\n",
       "        [ 5.3871e+00,  2.3279e+00,  3.1275e+00,  1.5623e+00],\n",
       "        [ 6.6405e+00,  2.8871e+00,  5.2787e+00,  8.2517e-01],\n",
       "        [ 6.8564e+00,  3.3128e+00,  5.2989e+00,  1.0717e+00],\n",
       "        [ 2.7674e+00,  2.7254e+00,  2.6531e+00,  2.3195e+00],\n",
       "        [ 6.6853e+00,  3.7157e+00,  5.2096e+00,  1.3785e+00]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward>)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals_roih_unsup_k[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
